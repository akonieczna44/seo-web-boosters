# podane strony nie miaÅ‚y moÅ¼liwoÅ›ci bezpoÅ›redniego pobrania z nich zdjÄ™Ä‡ - trzeba by wyciÄ…gaÄ‡ je bezpoÅ›rednio jako adresy url z kodu html, wchodziÄ‡ i dopiero zapisywaÄ‡
# dlaczego wiÄ™c nie napisaÄ‡ kodu, ktÃ³ry wyciÄ…ga takie adresy url obrazkÃ³w na serwerze, 
# wchodzi na te adresy url i dopiero stÄ…d je pobiera, 
# a nastÄ™pnie zapisuje zdjÄ™cia w folderze o nazwie ostatniego segmentu url?

import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin, urlparse
import re

# Pytamy uÅ¼ytkownika o URL
url = input("ğŸ”— Podaj URL strony z obrazkami: ").strip()

# Ostatni fragment Å›cieÅ¼ki URL jako nazwÄ™ folderu
path_parts = urlparse(url).path.rstrip("/").split("/")
folder_name = path_parts[-1] if path_parts[-1] else "pobrane_obrazki"
os.makedirs(folder_name, exist_ok=True)

# Pobieramy stronÄ™ i parsujemy
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")
img_tags = soup.find_all("img")

# Regex do czyszczenia miniaturek typu -1024x768
pattern = re.compile(r"-\d+x\d+(?=\.jpg)")

for img in img_tags:
    src = img.get("src")
    if src and ".jpg" in src.lower():
        full_url = urljoin(url, src)
        full_url_cleaned = pattern.sub("", full_url)

        filename = os.path.join(folder_name, os.path.basename(full_url_cleaned))
        print(f"â¬‡ï¸ Pobieram: {full_url_cleaned} -> {filename}")

        try:
            img_data = requests.get(full_url_cleaned).content
            with open(filename, "wb") as f:
                f.write(img_data)
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d przy pobieraniu {full_url_cleaned}: {e}")


# WyÅ›wietlam Å›cieÅ¼kÄ™ folderu i skryptu
script_path = os.getcwd()
full_path = os.path.abspath(folder_name)

print(f"\nğŸ“‚ Skrypt dziaÅ‚a w folderze:\n{script_path}")
print(f"âœ… Wszystkie obrazy zapisane w folderze:\n{full_path}")
